{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfabb23d-0e73-49cc-b9e5-4d6b413ccf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb import Client\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cf2aee-c9e4-4da8-a792-8498490c517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Haliç Üniversitesi Eğitim Ücretleri ve Bursları 2024-2025\\nHaliç Üniversitesinde eğitim-öğretim ücretlidir. 2024-2025 eğitim-öğretim yılı ücretleri,\\nHalkla İlişkiler ve Tanıtım (%25 İndirimli) 213.000\\x00    TL    (Ücretli) 284.000 TL\\nGastronomi ve Mutfak Sanatları (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nTürk Dili ve Edebiyatı (% 50 İndirimli) 142.000 TL (Ücretli) 284.000 TL\\nÇizgi Film ve Animasyon (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nGörsel İletişim Tasarımı (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nAmerikan Kültürü ve Edebiyatı (%25 İndirimli) 246.000 TL (Ücretli) 328.000 TL\\nİngilizce Mütercim ve Tercümanlık (%25 İndirimli) 246.000 TL (Ücretli) 328.000 TL\\nTekstil ve Moda Tasarımı (%25 İndirimli) 225.000 TL (Ücretli) 300.000 TL\\nAntrenörlük Eğitimi (%25 İndirimli) 240.750 TL (Ücretli) 321.000 TL\\nSpor Yöneticiliği (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nSpor Yöneticiliği (İngilizce) (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nBeden Eğitim ve Spor Öğretmenliği (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nRekreasyon (%25 İndirimli) 231.000 TL (Ücretli) 308.000 TL\\nOpera (%50 İndirimli) 140.000 TL (Ücretli) 280.000 TL\\nTiyatro (% 25 İndirimli) 210.000 TL (Ücretli) 280.000 TL\\nTürk Musikisi (%50 İndirimli) 140.000 TL (Ücretli) 280.000 TL\\nGrafik Tasarım (%25 İndirimli) 225.000 TL (Ücretli) 300.000 TL\\nİç Mimarlık ve Çevre Tasarımı (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nPsikoloji (%25 İndirimli) 258.000 TL (Ücretli) 344.000 TL\\nPsikoloji (İngilizce) (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nUluslararası Ticaret ve Finans (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nSiyaset Bilimi ve Uluslararası İlişkiler (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nİşletme (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nİşletme (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nYönetim Bilişim Sistemleri (İngilizce) (%25 İndirimli) 240.000 TL (Ücretli) 320.000 TL',\n",
       " 'Fizyoterapi ve Rehabilitasyon (%25 İndirimli) 225.000 TL (Ücretli) 300.000 TL\\nFizyoterapi ve Rehabilitasyon (İngilizce) (%25 İndirimli) 225.000 TL (Ücretli) 300.000 TL\\nHemşirelik (%25 İndirimli) 255.000 TL (Ücretli) 340.000 TL\\nHemşirelik (İngilizce) (%25 İndirimli) 255.000 TL (Ücretli) 340.000 TL\\nBeslenme ve Diyetetik (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nBeslenme ve Diyetetik (İngilizce) (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nEbelik (%25 İndirimli) 240.000 TL (Ücretli) 320.000 TL\\nYazılım Mühendisliği (İngilizce) (%25 İndirimli) 255.000 TL (Ücretli) 340.000 TL\\nElektrik-Elektronik Mühendisliği (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nEndüstri Mühendisliği (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nBilgisayar Mühendisliği (İngilizce) (%25 İndirimli) 255.000 TL (Ücretli) 340.000 TL\\nMakine Mühendisliği (İngilizce) (% 50 İndirimli) 142.000 TL (Ücretli) 284.000 TL\\nMatematik (%25 İndirimli) 225.000 TL (Ücretli) 300.000 TL\\nMoleküler Biyoloji ve Genetik (İngilizce) (%25 İndirimli) 246.000 TL (Ücretli) 328.000 TL\\nEndüstriyel Tasarım (%25 İndirimli) 210.000 TL (Ücretli) 280.000 TL\\nMimarlık (% 50 İndirimli) 146.000 TL (Ücretli) 292.000 TL\\nDijital Oyun Tasarımı (%25 İndirimli) 231.000 TL (Ücretli) 328.000 TL']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_PDF_Text(pdf_path=\"/Users/harun/Documents/GitHub/AI-Powered-Regional-Cost-of-Living-Advisor/Search/Rag/Uni_fiyatları/halicUniversitesi.pdf\"):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "    # Filter the empty strings\n",
    "    pdf_texts = [text for text in pdf_texts if text]\n",
    "    return pdf_texts\n",
    "\n",
    "pdf_texts=convert_PDF_Text()\n",
    "pdf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae257d4d-3199-4c67-af6c-b4f3969d92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for displaying text\n",
    "def to_markdown(text):\n",
    "    text = text.replace('•', '  *')\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a217ccf0-b1c7-495f-9824-c83687ef6413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Psikoloji (%25 İndirimli) 258.000 TL (Ücretli) 344.000 TL\\nPsikoloji (İngilizce) (%25 İndirimli) 270.000 TL (Ücretli) 360.000 TL\\nUluslararası Ticaret ve Finans (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nSiyaset Bilimi ve Uluslararası İlişkiler (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nİşletme (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nİşletme (İngilizce) (%25 İndirimli) 213.000 TL (Ücretli) 284.000 TL\\nYönetim Bilişim Sistemleri (İngilizce) (%25 İndirimli) 240.000 TL (Ücretli) 320.000 TL'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_Chunks_in_Char(pdf_texts, chunk_size=800, chunk_overlap=100):\n",
    "    # Combine all PDF texts into one string\n",
    "    full_text = '\\n\\n'.join(pdf_texts)\n",
    "\n",
    "    # Initialize sentence-based text splitter\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \":\", \";\", \",\", \" \", \"\"],  # Process in this order\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    # Split text into chunks based on sentences\n",
    "    chunks = sentence_splitter.split_text(full_text)\n",
    "    return chunks\n",
    "chunks=text_Chunks_in_Char(pdf_texts)\n",
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760452b0-93ee-4198-b80e-496dce6b8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = \"distiluse-base-multilingual-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "096c7df7-f363-4b8c-8cd3-6b702cdc5a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TL İşletme ( % 25 İndirimli ) 213. 000 TL ( Ücretli ) 284. 000 TL İşletme ( İngilizce ) ( % 25 İndirimli ) 213. 000 TL ( Ücretli ) 284. 000 TL Yönetim Bilişim Sistemleri ( İngilizce ) ( % 25 İndirimli ) 240. 000 TL ( Ücretli ) 320. 000 TL'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_Chunk_Token(text_chunksinChar, sentence_transformer_model, chunk_overlap=10, tokens_per_chunk=128):\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        model_name=sentence_transformer_model,\n",
    "        tokens_per_chunk=tokens_per_chunk)\n",
    "\n",
    "    text_chunksinTokens = []\n",
    "    for text in text_chunksinChar:\n",
    "        text_chunksinTokens += token_splitter.split_text(text)\n",
    "    print(f\"\\nTotal number of chunks (document splited by 128 tokens per chunk): {len(text_chunksinTokens)}\")\n",
    "    return text_chunksinTokens\n",
    "tokens=convert_Chunk_Token(chunks,sentence_transformer_model)\n",
    "tokens[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3470cc38-8fcd-4af7-b37d-97ba516ad7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = \"distiluse-base-multilingual-cased-v1\"\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=sentence_transformer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa0d4029-3746-4817-8a8c-f370241da507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_client(collection_name, embedding_function):\n",
    "    chroma_client = chromadb.PersistentClient()\n",
    "\n",
    "    # Check if collection already exists and delete it\n",
    "    existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "    if collection_name in existing_collections:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "\n",
    "    # Create a new collection\n",
    "    chroma_collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    return chroma_client, chroma_collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f439905-c0e9-4558-bf4d-de50fe058983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_data(chunks, title, category, initial_id):\n",
    "    ids = [str(i + initial_id) for i in range(len(chunks))]\n",
    "    filename = os.path.basename(title)\n",
    "    metadata = {\n",
    "        'document': filename,\n",
    "        'category': category\n",
    "    }\n",
    "    metadatas = [metadata for i in range(len(chunks))]\n",
    "    return ids, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0da4b6f-c7a3-4bfd-b40d-cf43d601bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_collection(ids, metadatas, chunks, chroma_collection):\n",
    "    chroma_collection.upsert(ids=ids, metadatas=metadatas, documents=chunks)\n",
    "    return chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2fe008f-c5b9-4fad-bc9e-c863a1ba01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveDocs(chroma_collection, query, file=None, n_results=10, return_only_docs=False):\n",
    "\n",
    "    # Build query parameters\n",
    "    query_params = {\n",
    "        \"query_texts\": [query],\n",
    "        \"include\": [\"documents\", \"metadatas\", 'distances'],\n",
    "        \"n_results\": n_results\n",
    "    }\n",
    "\n",
    "    # Add file filter if provided\n",
    "    if file is not None:\n",
    "        query_params[\"where\"] = {\"document\": f\"{file}.pdf\"}\n",
    "\n",
    "    # Execute the query\n",
    "    results = chroma_collection.query(**query_params)\n",
    "\n",
    "    if return_only_docs:\n",
    "        return results['documents'][0]\n",
    "    else:\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a74b898-a6db-4ed8-be3b-b8792f6dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(results, return_only_docs=False):\n",
    "    if return_only_docs:\n",
    "        retrieved_documents = results\n",
    "        if len(retrieved_documents) == 0:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        for i, doc in enumerate(retrieved_documents):\n",
    "            print(f\"Document {i + 1}:\")\n",
    "            print(\"\\tDocument Text: \")\n",
    "            pprint(doc)\n",
    "    else:\n",
    "        retrieved_documents = results['documents'][0]\n",
    "        if len(retrieved_documents) == 0:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        retrieved_documents_metadata = results['metadatas'][0]\n",
    "        retrieved_documents_distances = results['distances'][0]\n",
    "        print(\"------- Retrieved documents -------\\n\")\n",
    "\n",
    "        for i, doc in enumerate(retrieved_documents):\n",
    "            print(f\"Document {i + 1}:\")\n",
    "            print(\"\\tDocument Text: \")\n",
    "            pprint(doc)\n",
    "            # Extract filename from the full path\n",
    "            full_path = retrieved_documents_metadata[i]['document']\n",
    "            filename = os.path.basename(full_path)\n",
    "            print(f\"\\tDocument Source: {filename}\")\n",
    "            print(f\"\\tDocument Source Type: {retrieved_documents_metadata[i]['category']}\")\n",
    "            print(f\"\\tDocument Distance: {retrieved_documents_distances[i]}\")\n",
    "            print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a06297e-9bac-4759-8b11-37602739f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    all_entries = os.listdir(directory_path)\n",
    "\n",
    "    files_only = [entry for entry in all_entries\n",
    "                  if os.path.isfile(os.path.join(directory_path, entry))]\n",
    "    return files_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e172ccfd-88cc-426f-906b-f34a2f6a9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pdf_paths(\n",
    "        directory_path=\"/Users/harun/Documents/GitHub/AI-Powered-Regional-Cost-of-Living-Advisor/Search/Rag/Uni_fiyatları\"):\n",
    "    \"\"\"\n",
    "    Get all PDF file paths from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing PDF files\n",
    "\n",
    "    Returns:\n",
    "        list: List of full paths to all PDF files in the directory\n",
    "    \"\"\"\n",
    "    all_files = list_files_in_directory(directory_path)\n",
    "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
    "    full_paths = [os.path.join(directory_path, pdf_file) for pdf_file in pdf_files]\n",
    "    return full_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72429681-e5da-41a6-af56-84949a2e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_pdfs_to_ChromaDB(collection_name, sentence_transformer_model):\n",
    "    \"\"\"\n",
    "    Load multiple PDFs into ChromaDB collection with proper chunking and embedding.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the ChromaDB collection\n",
    "        sentence_transformer_model (str): Name of the sentence transformer model to use\n",
    "\n",
    "    Returns:\n",
    "        tuple: (chroma_client, chroma_collection)\n",
    "    \"\"\"\n",
    "    # Initialize embedding function\n",
    "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=sentence_transformer_model)\n",
    "\n",
    "    # Create or get ChromaDB client and collection\n",
    "    chroma_client, chroma_collection = create_chroma_client(collection_name, embedding_function)\n",
    "\n",
    "    # Get current ID for continuous indexing\n",
    "    current_id = chroma_collection.count()\n",
    "\n",
    "    # Get all PDF paths from the directory\n",
    "    pdf_paths = get_all_pdf_paths()\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        # Extract text from PDF\n",
    "        pdf_texts = convert_PDF_Text(pdf_path)\n",
    "\n",
    "        # Create character-based chunks first\n",
    "        char_chunks = text_Chunks_in_Char(pdf_texts)\n",
    "        \n",
    "        # Convert to token-based chunks\n",
    "        token_chunks = convert_Chunk_Token(char_chunks, sentence_transformer_model)\n",
    "\n",
    "        # Add metadata and get IDs\n",
    "        ids, metadatas = add_meta_data(token_chunks, pdf_path, \"PricePaper\", current_id)\n",
    "\n",
    "        # Update current_id for next document\n",
    "        current_id += len(token_chunks)\n",
    "\n",
    "        # Add to collection\n",
    "        chroma_collection = add_document_to_collection(ids, metadatas, token_chunks, chroma_collection)\n",
    "\n",
    "    return chroma_client, chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58d324c1-b7b7-4bce-9cee-7cd4de5675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_chroma_collection(collection_name):\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient()\n",
    "    \n",
    "    # Get the existing collection\n",
    "    chroma_collection = chroma_client.get_collection(\n",
    "        name=collection_name\n",
    "    )\n",
    "    \n",
    "    return  chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afcebb88-1b77-446f-9ec4-7ab3cb090edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 1\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 16\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 28\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 16\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 20\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 9\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 23\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 11\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 18\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 18\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 15\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 66\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 1\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 1\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 5\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 53\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 21\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 5\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 11\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 10\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 5\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 8\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 8\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 4\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 9\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 24\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 7\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 14\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 19\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 17\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 3\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 27\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 2\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 5\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 10\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 19\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 17\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 7\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 25\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 11\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 8\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 8\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 22\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 4\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 4\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 16\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 14\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 8\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 17\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 10\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 7\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 4\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 41\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 9\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 6\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 14\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 5\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 10\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 6\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 40\n",
      "\n",
      "Total number of chunks (document splited by 128 tokens per chunk): 35\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keyword_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#chroma_collection=get_existing_chroma_collection(\"UniPrices\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaliç Üniversitesi Tiyatro\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m keyword \u001b[38;5;241m=\u001b[39m \u001b[43mkeyword_agent\u001b[49m\u001b[38;5;241m.\u001b[39mparse_keywords(query)\n\u001b[1;32m      7\u001b[0m filtered_keyword \u001b[38;5;241m=\u001b[39m keyword\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_keyword \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keyword_agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "chroma_client, chroma_collection = load_multiple_pdfs_to_ChromaDB(\"UniPrices\", sentence_transformer_model)\n",
    "#chroma_collection=get_existing_chroma_collection(\"UniPrices\")\n",
    "\n",
    "query = \"Haliç Üniversitesi Tiyatro\"\n",
    "keyword = keyword_agent.parse_keywords(query)\n",
    "filtered_keyword = keyword.strip('\"')\n",
    "if filtered_keyword != 'None':\n",
    "    retrieved_documents = retrieveDocs(chroma_collection, query, \"halicUniversitesi\", 10)\n",
    "    show_results(retrieved_documents)\n",
    "else:\n",
    "    retrieved_documents = retrieveDocs(chroma_collection, query, None, 10)\n",
    "    show_results(retrieved_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232eb70-0f97-430d-ba38-e9d2c4d15eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
